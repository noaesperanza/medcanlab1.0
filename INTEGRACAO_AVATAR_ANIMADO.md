# üé≠ INTEGRA√á√ÉO AVATAR ANIMADO - N√îA ESPERAN√áA

## üìã **SITUA√á√ÉO ATUAL**

Temos **duas rotas** relacionadas √† N√¥a:

1. **Perfil da N√¥a** (`/app/chat-noa-esperanca`)
   - Interface est√°tica
   - Apresenta funcionalidades
   - √çcones de caracter√≠sticas

2. **Chat Flutuante** (`NoaPlatformChat.tsx`)
   - Intera√ß√£o em tempo real
   - Texto puro
   - Acesso em todas as rotas

---

## üéØ **OBJETIVO: UNIFICAR E ANIMAR**

### **Fase 1: Unificar Rotas (Atual)**

Mesclar as duas interfaces em uma √∫nica experi√™ncia:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  N√îA ESPERAN√áA - IA RESIDENTE           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Avatar        ‚îÇ  ‚îÇ   Chat        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Animado       ‚îÇ  ‚îÇ   Texto       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   (Pensando)    ‚îÇ  ‚îÇ   (Mensagens) ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Fase 2: Avatar Animado (Pr√≥xima)**

Adicionar:
- ‚úÖ Olhos se movendo
- ‚úÖ Boca sincronizada com voz
- ‚úÖ Express√µes faciais
- ‚úÖ √Åudio TTS (Text-to-Speech)

---

## üöÄ **PLANO DE IMPLEMENTA√á√ÉO**

### **FASE 1: UNIFICAR INTERFACES** (Agora)

#### 1.1 **Criar Componente Unificado**

```typescript
// src/components/NoaUnifiedInterface.tsx
export const NoaUnifiedInterface = () => {
  return (
    <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
      {/* Avatar Section */}
      <NoaAvatarSection />
      
      {/* Chat Section */}
      <NoaChatSection />
    </div>
  )
}
```

#### 1.2 **Avatar Section**

Mostrar avatar est√°tico (por enquanto):

```typescript
const NoaAvatarSection = () => {
  return (
    <div className="bg-slate-800 rounded-xl p-6 border border-slate-700">
      <div className="flex flex-col items-center">
        {/* Avatar Image */}
        <div className="w-32 h-32 rounded-full bg-gradient-to-br from-purple-500 to-pink-500 mb-4">
          <img src="/noa-avatar.png" alt="N√¥a" className="w-full h-full rounded-full" />
        </div>
        
        {/* Name */}
        <h2 className="text-2xl font-bold text-white mb-2">N√¥a Esperan√ßa</h2>
        
        {/* Specialization */}
        <p className="text-slate-400 text-sm text-center mb-6">
          IA Residente especializada em Cannabis Medicinal e Nefrologia
        </p>
        
        {/* Capabilities */}
        <div className="grid grid-cols-2 gap-4 w-full">
          <CapabilityCard icon="üíñ" title="An√°lise Emocional" />
          <CapabilityCard icon="üß†" title="Diagn√≥stico IA" />
          <CapabilityCard icon="üë§" title="Suporte M√©dico" />
          <CapabilityCard icon="‚ö°" title="Mem√≥ria Persistente" />
        </div>
      </div>
    </div>
  )
}
```

---

### **FASE 2: ANIMAR AVATAR** (Pr√≥xima)

#### 2.1 **Bibliotecas Necess√°rias**

```bash
npm install @lottiefiles/react-lottie-player
# OU
npm install ready-player-me
# OU usar Canvas API + anima√ß√µes customizadas
```

#### 2.2 **Estados do Avatar**

```typescript
type AvatarState = 
  | 'idle'      // Parado, olhando
  | 'thinking'  // Pensando (movendo olhos)
  | 'speaking'  // Falando (boca se movendo)
  | 'listening' // Ouvindo (inclinado)
```

#### 2.3 **Sincronizar com TTS**

```typescript
const playAudioWithLipSync = async (text: string) => {
  // 1. Gerar √°udio TTS (OpenAI TTS API)
  const audio = await generateTTS(text)
  
  // 2. Detectar fones (phonemes) para sincroniza√ß√£o
  const phonemes = await detectPhonemes(text)
  
  // 3. Animar boca baseado nos fones
  animator.syncLipSync(audio, phonemes)
  
  // 4. Reproduzir √°udio
  audio.play()
  
  // 5. Retornar para estado idle quando terminar
  setTimeout(() => setAvatarState('idle'), audio.duration)
}
```

---

## üé® **ESTADOS DO AVATAR**

### **1. Idle (Parado)**
- Olhos movendo suavemente
- Piscando ocasionalmente
- Respira√ß√£o sutil

### **2. Thinking (Pensando)**
- Olhos se movendo mais r√°pido
- Cabe√ßa levemente inclinada
- Indicador de "processando"

### **3. Speaking (Falando)**
- Boca sincronizada com fala
- Movimentos de express√£o
- Gestos sutis

### **4. Listening (Ouvindo)**
- Cabe√ßa inclinada para frente
- Olhos fixos na c√¢mera/mic
- Indica√ß√£o visual de "ouvindo"

---

## üì¶ **IMPLEMENTA√á√ÉO T√âCNICA**

### **Op√ß√£o 1: Lottie Animations** (Mais Simples)

```typescript
import Lottie from 'react-lottie-player'

const NoaAnimatedAvatar = ({ state }: { state: AvatarState }) => {
  const animation = {
    idle: require('@/animations/noa-idle.json'),
    thinking: require('@/animations/noa-thinking.json'),
    speaking: require('@/animations/noa-speaking.json'),
    listening: require('@/animations/noa-listening.json'),
  }

  return (
    <Lottie
      loop={state !== 'idle'}
      animationData={animation[state]}
      play
      style={{ width: 300, height: 300 }}
    />
  )
}
```

### **Op√ß√£o 2: Three.js / Ready Player Me** (Mais Avan√ßado)

```typescript
import { Canvas } from '@react-three/fiber'
import Avatar3D from './Avatar3D'

const Noa3DAvatar = ({ state }: { state: AvatarState }) => {
  return (
    <Canvas camera={{ position: [0, 1.6, 2] }}>
      <ambientLight />
      <pointLight position={[10, 10, 10]} />
      <Avatar3D state={state} />
    </Canvas>
  )
}
```

### **Op√ß√£o 3: Canvas API + Animations** (Custom)

```typescript
const drawAvatar = (ctx: CanvasRenderingContext2D, state: AvatarState) => {
  // Desenhar rosto base
  drawFace(ctx)
  
  // Animar olhos
  animateEyes(ctx, state)
  
  // Animar boca (se falando)
  if (state === 'speaking') {
    animateMouth(ctx)
  }
}
```

---

## üîä **INTEGRA√á√ÉO COM AUDIO**

### **Text-to-Speech (TTS)**

```typescript
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

const speakText = async (text: string) => {
  // Gerar √°udio com OpenAI TTS
  const response = await openai.audio.speech.create({
    model: 'tts-1',
    voice: 'nova', // Voz feminina suave
    input: text,
  })
  
  // Converter para blob
  const audioBlob = await response.blob()
  const audioUrl = URL.createObjectURL(audioBlob)
  
  // Reproduzir
  const audio = new Audio(audioUrl)
  audio.play()
  
  return audio
}
```

---

## üéØ **PR√ìXIMOS PASSOS**

### **Imediato (Hoje)**
- [ ] Unificar as duas rotas da N√¥a
- [ ] Criar componente `NoaUnifiedInterface`
- [ ] Migrar chat flutuante para interface unificada

### **Curto Prazo (Esta Semana)**
- [ ] Adicionar avatar est√°tico na interface unificada
- [ ] Implementar estados b√°sicos (idle, thinking)
- [ ] Criar anima√ß√µes Lottie para estados

### **M√©dio Prazo (Pr√≥ximas 2 Semanas)**
- [ ] Integrar TTS (Text-to-Speech)
- [ ] Sincronizar anima√ß√£o de boca com √°udio
- [ ] Adicionar express√µes faciais
- [ ] Implementar eye tracking (seguir cursor)

### **Longo Prazo (M√™s)**
- [ ] Avatar 3D (Ready Player Me ou custom)
- [ ] Sincroniza√ß√£o avan√ßada de l√°bios
- [ ] Gestos e movimentos corporais
- [ ] Reconhecimento de emo√ß√µes do usu√°rio

---

## üí° **EXEMPLO DE USO**

```typescript
// src/components/NoaUnifiedInterface.tsx
const [avatarState, setAvatarState] = useState<AvatarState>('idle')

const handleUserMessage = async (message: string) => {
  // 1. Avatar muda para "thinking"
  setAvatarState('thinking')
  
  // 2. Processar mensagem
  const response = await noaAssistant.sendMessage(message)
  
  // 3. Avatar muda para "speaking"
  setAvatarState('speaking')
  
  // 4. Gerar e reproduzir √°udio + anima√ß√£o
  await speakWithAnimation(response.text)
  
  // 5. Avatar volta para "idle"
  setAvatarState('idle')
}
```

---

## üöÄ **TESTE R√ÅPIDO**

Para testar as anima√ß√µes b√°sicas:

1. Criar pasta `src/animations/`
2. Adicionar JSONs do Lottie
3. Implementar componente b√°sico
4. Testar estados

---

**Vers√£o:** 1.0.0  
**Data:** Janeiro 2025  
**Status:** Planejamento
